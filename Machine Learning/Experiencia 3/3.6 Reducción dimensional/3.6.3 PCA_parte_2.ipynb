{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ACHy-Jxa0uWQ"},"source":["# Descomposición en componentes principales (PCA) - PARTE 2\n","\n","En este notebook vamos a explorar una aplicacion de PCA q.\n","\n","La idea de componentes principales (y de muchas otras técnicas de reducción dimensional) es encontrar una combinación de los features originales que condensen gran parte de la variabilidad de nuestros datos. La utilidad de esto radica en poder:\n","- visualizar los datos en un espacio mucho más pequeño que el espacio original;\n","- encontrar direcciones que condensen la variación de features fuertemente correlacionados y, por lo tanto, eliminar información redundante;\n","- alimentar modelos de regresión o clasificación con menos variables independientes;\n","- comprimir información - **aplicación de hoy**.\n","\n","La descomposición en componentes principales es parte del conjunto de algoritmos conocidos como de **aprendizaje no-supervisado**. Esto se debe a que estos algoritmos trabajan sobre el conjunto de features, sin que exista una variable que querramos predecir (variable *target*)."]},{"cell_type":"code","metadata":{"id":"fgYY1u_Wx5Zp"},"source":["# Librerias generales\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gM3QU5UU3viD"},"source":["Importemos la clase que nos va a permitir realizar PCA, además del módulo para estandarizar nuestras variables, que suele ser una práctica habitual antes de aplicar PCA."]},{"cell_type":"code","metadata":{"id":"8a02NBGQ3wSd"},"source":["# Clase para realizar componentes principales\n","from sklearn.decomposition import PCA\n","\n","# Estandarizador (transforma las variables en z-scores)\n","from sklearn.preprocessing import StandardScaler\n","std_scale = StandardScaler() # Creamos el estandarizador"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2la5yj239Z6"},"source":["##Trabajaremos con el dataset  de caras: Olivetti"]},{"cell_type":"code","metadata":{"id":"3bod9bbL39jc"},"source":["from sklearn.datasets import fetch_olivetti_faces # para cargar el dataset de caras\n","data, targets = fetch_olivetti_faces(return_X_y = True) # cargamos las caras\n","\n","print('Dimensión de los datos {}'.format(data.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7lkJZ4h4d3-"},"source":["Vemos algunas imágenes"]},{"cell_type":"code","metadata":{"id":"e-JqVTVI4gTI"},"source":["# Ploteo 25 imagenes al azar\n","fig = plt.figure(figsize = (12,12)) # seteo el tamano de la figura\n","for i in range(25):\n","    j = np.random.randint(0, data.shape[0]) # en cada iteracion elijo un numero random entre 0 y el numero de imágenes\n","    plt.subplot(5,5,i+1) # Voy a tener una matriz de 5x5 subplots y voy llenando en la iteracion i-esima el subplot i+1\n","    plt.imshow(data[j,:].reshape(64,64), interpolation='none', cmap=\"gray\") # plotea una imagen random, pues es la imagen j-esima del set de entrenamiento, en formato (28,28) para imágenes en escala de grises (tengo que reshapear)\n","    plt.title(\"Persona: {}\".format(targets[j]), fontsize = 10) # pongo el titulo a los plots con el identificador unico de la persona\n","    plt.xticks([]) # le saco los ticks en el eje X\n","    plt.yticks([]) # le saco los ticks en el eje Y\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYUtkyxy6DG3"},"source":["### Visualización en el espacio bidimensional de las componentes principales\n","\n","Cada una de las caras está descrito por un vector de $4096$ píxeles. Es decir, podemos pensar nuestro dataset como compuesto por $400$ instancias en un espacio de $4096$ features. Debido a que mucho de estos features pueden estar fuertemente correlacionados (pensar en píxeles adyacentes) podemos quizás encontrar combinación de los mismos que nos permita reducir el espacio en el cual viven nuestros datos.\n","\n","Veamos qué pasa si esa reducción la hacemos en el espacio de las dos primeras componentes principales\n","\n","¿hace falta estandarizar las variables en este problema?\n","\n","Discuta en clases su respuesta antes de seguir"]},{"cell_type":"code","metadata":{"id":"xswn6f6-6lI8"},"source":["# Creación del modelo de PCA con 2 componentes\n","pca = PCA(n_components = 2)\n","\n","# Ajuste y transformación de los datos\n","pca.fit(data)\n","X_pca = pca.transform(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJiEkP6-6qqg"},"source":["Veamos cómo se ubican cada uno de los datos en este espacio:"]},{"cell_type":"code","metadata":{"id":"6KxQlRTx6tMa"},"source":["fig, ax = plt.subplots(figsize = (20, 7))\n","\n","# Hacemos un scatter plot de cada uno de los datos\n","ax.scatter(X_pca[:, 0], X_pca[:, 1])\n","\n","# Por cada dato escribimos a qué instancia corresponde\n","for i in range(data.shape[0]):\n","  ax.text(X_pca[i, 0], X_pca[i, 1], s = i)\n","\n","ax.set_xlabel('Primer componente principal')\n","ax.set_ylabel('Segunda componente principal')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtUWh9KK8GAX"},"source":["Seleccionemos algunos puntos que estén muy distantes entre si y veamos a qué caras corresponden:"]},{"cell_type":"code","metadata":{"id":"Q_75AQDZ8Gtl"},"source":["# Lista de índices de caras\n","faceids = [169, 336, 76, 55]\n","\n","fig, ax = plt.subplots(1, 4, figsize = (15, 5)) # seteo el tamano de la figura\n","\n","# muestra la imagen de índice faceids[j], en formato (28,28) para imágenes en escala de grises (tenemos que aplicar .reshape())\n","ax[0].imshow(data[faceids[0], :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","ax[1].imshow(data[faceids[1], :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","ax[2].imshow(data[faceids[2], :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","ax[3].imshow(data[faceids[3], :].reshape(64, 64), interpolation='none', cmap=\"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FKDPfNhP8ohK"},"source":["**Pruebe con otros faceids**"]},{"cell_type":"markdown","metadata":{"id":"xngQchsV80Qp"},"source":["Veamos ahora algunos puntos que estén muy cerca entre si:"]},{"cell_type":"code","metadata":{"id":"CKXfsBFg81b3"},"source":["# Lista de índices de caras\n","faceids = [55, 56, 57, 58]\n","\n","fig, ax = plt.subplots(1, 4, figsize = (15, 5)) # seteo el tamano de la figura\n","\n","# muestra la imagen de índice faceids[j], en formato (28,28) para imágenes en escala de grises (tenemos que aplicar .reshape())\n","ax[0].imshow(data[faceids[0], :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","ax[1].imshow(data[faceids[1], :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","ax[2].imshow(data[faceids[2], :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","ax[3].imshow(data[faceids[3], :].reshape(64, 64), interpolation='none', cmap=\"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4hVvCwBG9IVy"},"source":["**Pruebe con otros faceids**"]},{"cell_type":"markdown","metadata":{"id":"ftHZ7RD79lCg"},"source":["¿Qué concluímos con esto último? Reducir la dimensión en la que viven nuestros datos de forma inteligente (como con componentes principales) acerca instancias que son parecidas entre si, acercándonos a nosotros a la idea de *clustering*."]},{"cell_type":"markdown","metadata":{"id":"hajz1rOf91LH"},"source":["### Fracción de varianza explicada\n","\n","Elegimos empezar en forma arbitraria con un espacio bidimensional, pero cuánta información estamos reteniendo en ese espacio? Exploremos cuánto aporta cada componente principal, yéndonos ahora hasta 100 componentes (manteniéndonos siempre en un zona donde la cantidad de componentes principales sea bastante más chica que la dimensión del espacio original):"]},{"cell_type":"code","metadata":{"id":"oJ_f5w5Q988A"},"source":["# Creación del modelo de PCA con 100 componentes\n","pca = PCA(n_components=100)\n","\n","# Ajuste y transformación de los datos\n","pca.fit(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hi7ctIWG-AEU"},"source":["Grafiquemos la información que aporta cada componente y la información acumulada:"]},{"cell_type":"code","metadata":{"id":"6PVQq974-DOG"},"source":["# con .explained_variance_ratio_ vemos la fracción de información que aporta cada componente\n","evr = pca.explained_variance_ratio_\n","\n","# Graficamos la fracción de varianza que aporta cada componente y la información acumulada\n","fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n","\n","ax[0].plot(range(1, len(evr) + 1), evr, '-', linewidth = 2)\n","ax[0].set_ylabel('Fracción de varianza explicada')\n","ax[0].set_xlabel('Número de componentes principal')\n","\n","# Calculamos el acumulado con la función cumsum de numpy\n","varianza_acumulada = np.cumsum(evr)\n","\n","ax[1].plot(range(1, len(evr) + 1), varianza_acumulada, '-', linewidth = 2)\n","ax[1].set_ylabel('Fracción acumulada de varianza explicada')\n","ax[1].set_xlabel('Cantidad de componentes principales')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CgfGeiEx_TGY"},"source":["Notemos que a pesar que nuestros datos viven originalmente en un espacio de $4096$ features (píxeles) con solo 60 componentes principales alcanzamos alrededor del 90% de la información, lo cual implica una reducción dimensional bastante importante sin una pérdida tan significativa de la información."]},{"cell_type":"markdown","metadata":{"id":"ABlJ3NON_X5g"},"source":["### Visualización de las componentes principales\n","\n","Veamos cómo se visualizan las primeras componentes en el espacio de features originales:"]},{"cell_type":"code","metadata":{"id":"fmxXsssb_fQC"},"source":["fig, ax = plt.subplots(1, 5, figsize = (20, 10)) # seteo el tamano de la figura\n","\n","# Hacemos un loop sobre las primeras 5 componentes\n","for pc in range(5):\n","\n","  # Ploteo de la cara reconstruida con el mismo código de antes\n","  # plotea la image dada por pc-componente principal en formato (28,28) para imagenes en escala de grises (tengo que aplicar reshape())\n","  ax[pc].imshow(pca.components_[pc].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","\n","  ax[pc].set_title(\"Componente {}\".format(pc + 1), fontsize = 10) # pongo el titulo a los plots con el componente\n","\n","  ax[pc].set_xticks([]) # le saco los ticks en el eje X\n","  ax[pc].set_yticks([]) # le saco los ticks en el eje Y\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5J2top3cALI6"},"source":["Lo que vemos es que las componentes ya remiten a las figuras originales, es decir, detectan cierto patrón global (otros algoritmos más sofisticados utilizados para la identificación de caras podrían detectar elementos aislados tales como un ojo, una nariz, una boca, etc., y así reconstruir la cara original)."]},{"cell_type":"markdown","metadata":{"id":"zCu7EjHkAjZF"},"source":["### Reconstrucción de los datos a partir del espacio reducido\n","\n","Empecemos seleccionando dicho ejemplo (pueden elegir el que quieran):"]},{"cell_type":"code","metadata":{"id":"s_7gwBhcAMA5"},"source":["# Elegimos alguna instancia en particular\n","faceid = 23\n","\n","fig, ax = plt.subplots(figsize = (5, 5)) # seteo el tamano de la figura\n","\n","# mostramos la imagen de índice faceid en formato (28,28) para imagenes en escala de grises (tengo que reshapear)\n","ax.imshow(data[faceid, :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","\n","ax.set_title(\"Persona: {}\".format(targets[faceid]), fontsize = 10) # pongo el titulo a los plots con el identificador unico de la persona\n","ax.set_xticks([]) # le saco los ticks en el eje X\n","ax.set_yticks([]) # le saco los ticks en el eje Y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQwy4Qy6Gb2H"},"source":["Veamos ahora cómo se ve la cara reconstruída cuando intentamos recuperar el espacio original con solo unas pocas componentes principales. Para reconstruir los datos desde el espacio reducido vamos a llamar al método:\n","~~~\n","pca.inverse_transform(X_pca)\n","~~~\n","Veamos la cara reconstruída desde el espacio de algunas componentes:"]},{"cell_type":"code","metadata":{"id":"oHWDksIsGgSf"},"source":["fig, ax = plt.subplots(1, 5, figsize = (20, 10)) # seteo el tamano de la figura\n","\n","fig_index = 0 # Indice para indicar donde hacer la figura\n","\n","# Recorremos distinta cantidad de componentes\n","for p in [2, 10, 25, 50, 100]:\n","\n","  # Creación del modelo de PCA con p componentes\n","  pca = PCA(n_components = p)\n","\n","  # Ajuste y transformación de los datos\n","  pca.fit(data)\n","  X_pca = pca.transform(data)\n","\n","  # Reconstrucción de la cara desde el espacio reducido!!!\n","  X_r = pca.inverse_transform(X_pca)\n","\n","  # Mostramos la cara reconstruida con el mismo código de antes\n","  ax[fig_index].imshow(X_r[faceid, :].reshape(64, 64), interpolation='none', cmap=\"gray\")\n","\n","  ax[fig_index].set_title(\"# componentes principales: {}\".format(p), fontsize = 10)\n","\n","  ax[fig_index].set_xticks([]) # le saco los ticks en el eje X\n","  ax[fig_index].set_yticks([]) # le saco los ticks en el eje Y\n","\n","  fig_index += 1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3JgbDEcUHQcv"},"source":["¿Qué vemos? Cuanto más componentes principales hayamos preservado mejor será la reproducción de la cara original (ver en la figura de varianza explicada cuánta información retuvimos en cada caso). Sin embargo con pocas componentes (con muchas menos que la cantidad de features del espacio original) ya logramos imágenes bastantes fieles de la imagen original."]},{"cell_type":"markdown","metadata":{"id":"qM13r6vlHdvM"},"source":["# Resumen de las cosas importantes\n","\n","Lo nuevo de esta notebook es el método para reconstruir el espacio original con algunas componentes principales. Si las componentes principales son menos que la cantidad de features del espacio original, en el medio habremos perdido información. Sin embargo es una técnica para comprimir precisamente la información.\n","\n","Para tratar de recuperar el espacio original luego de ajustar sería entonces:\n","~~~\n","from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components = p)\n","pca.fit(X)\n","X_pca = pca.transform(X)\n","\n","# X reconstruida\n","X_r = pca.inverse_transform(X_pca)\n","~~~"]},{"cell_type":"markdown","metadata":{"id":"-YoPDKeiHkWu"},"source":["Por último, en esta notebook no hizo falta escalear las variables ya que todos los píxeles tienen el mismo rango. Si estuvieramos en un caso donde escaleamos, para reconstruir el espacio original tenemos que además anti-transformar el escaleo:\n","~~~\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","std_scale = StandardScaler()\n","\n","std_scale.fit(X)\n","X_scaled = std_scale.transform(X)\n","\n","pca = PCA(n_components = p)\n","pca.fit(X_scaled)\n","X_pca = pca.transform(X_scaled)\n","\n","# X reconstruida pero aun escaleada\n","X_r_scaled = pca.inverse_transform(X_pca)\n","\n","# X reconstruida invirtiendo el escaleo\n","X_r = std_scale.inverse_transform(X_r_scaled)\n","~~~"]},{"cell_type":"markdown","metadata":{"id":"UWi5DSqR1lGp"},"source":["#Ejercite lo recien visto probando lo siguiente:\n","\n","\n","1.   Reconstruya las imágenes con más componentes principales. A su juicio, ¿cuál seria el valor ideal?\n","2.   Obtenga la varianza explicada con esas componentes principales.\n","\n"]}]}